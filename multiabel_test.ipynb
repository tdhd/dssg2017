{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score,precision_score,recall_score,f1_score\n",
    "from scipy.sparse import hstack\n",
    "import warnings,json,gzip\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def classify_cancer(fn, penalty, alpha):\n",
    "    '''\n",
    "    Runs a multilabel classification experiment\n",
    "    '''\n",
    "    X,y,labelNames = getFeaturesAndLabelsFine(fn)\n",
    "    print X.shape, y.shape, len(labelNames)\n",
    "    # a train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    # turn off warnings, usually there are some labels missing in the training set\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # train a classifier\n",
    "        print(\"Training classifier\")\n",
    "        classif = OneVsRestClassifier(SGDClassifier(penalty=penalty, alpha=alpha), n_jobs=-1).fit(X_train, y_train)\n",
    "    # predict\n",
    "    y_predicted = classif.predict(X_test)\n",
    "    # the scores we want to compute\n",
    "    scorers = [precision_score,recall_score,f1_score]\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # compute Scores\n",
    "        metrics = {s.__name__:getSortedMetrics(y_test,y_predicted,labelNames,s) for s in scorers}\n",
    "    # dump results\n",
    "    json.dump(metrics,gzip.open(\"multilabel_classification_metrics.json\",\"wt\"))\n",
    "    hl = hamming_loss(y_test,y_predicted)\n",
    "    ps = precision_score(y_test, y_predicted, average='samples')\n",
    "    rs = recall_score(y_test, y_predicted, average='samples')\n",
    "    return hl, ps, rs\n",
    "\n",
    "# medicinal database indicators\n",
    "def indicator_for(row):\n",
    "    words = ['medline', 'pubmed', 'embase', 'cochrane', 'cochrane library', 'ovid', 'google scholar']\n",
    "    return [1.0 if row['review_article'] == 1 and word.lower() in row.abstract.lower().split(\" \") else 0.0 for word in words]\n",
    "\n",
    "def indicator_count_for(row):\n",
    "    words = ['medline', 'pubmed', 'embase', 'cochrane', 'cochrane library', 'ovid', 'google scholar']\n",
    "    return np.sum([1.0 if row['review_article'] == 1 and word.lower() in row.abstract.lower().split(\" \") else 0.0 for word in words])\n",
    "\n",
    "def one_hot_database_indicators_from(df):\n",
    "    return OneHotEncoder().fit_transform(df.fillna(\"\").apply(indicator_count_for, axis=1).values.reshape(-1, 1))\n",
    "\n",
    "def getFeatures(fn):\n",
    "    '''\n",
    "    Load and vectorize features\n",
    "    '''\n",
    "    df = pd.read_csv(fn)\n",
    "    features = []\n",
    "    print(\"Vectorizing title character ngrams\")\n",
    "    titleVectorizer = HashingVectorizer(analyzer=\"char_wb\",ngram_range=(1,4),n_features=2**12)\n",
    "    features.append(titleVectorizer.fit_transform(df.fulltitle.fillna(\"\")))\n",
    "    print(\"Vectorizing keywords\")\n",
    "    # searchquery_terms is already a preprocessing step done by data angels, for direct integration\n",
    "    # keywords seems to be better\n",
    "    #features.append(CountVectorizer().fit_transform(df.searchquery_terms.str.replace('[\\[\\]\\'\\\"]',\"\")))\n",
    "    # take original keywords as per search query\n",
    "    features.append(CountVectorizer().fit_transform(df.keywords.str.replace('[\\[\\]\\'\\\"]',\"\")))\n",
    "    print(\"Vectorizing authors\")\n",
    "    features.append(HashingVectorizer(n_features=2**12).fit_transform(df.author.fillna(\"\").str.replace('[\\[\\]\\'\\\"]',\"\")))\n",
    "    print(\"Vectorizing abstracts\")\n",
    "    features.append(HashingVectorizer(n_features=2**12).fit_transform(df.abstract.fillna(\"\").str.replace('[\\[\\]\\'\\\"]',\"\")))\n",
    "    print(\"Computing medicinal database counts\")\n",
    "    features.append(one_hot_database_indicators_from(df))\n",
    "    X = hstack(features)\n",
    "    print(\"Extracted feature vectors with %d dimensions\"%X.shape[-1])\n",
    "    return X\n",
    "\n",
    "def getFeaturesAndLabelsFineMapped(fn):\n",
    "    '''\n",
    "    TODO\n",
    "    Load and vectorizer features and fine grained labels (vectorized using MultiLabelBinarizer)\n",
    "    Before piping through MultiLabelBinarizer, apply mapping to reduce the cardinality of labels\n",
    "    '''\n",
    "    print(\"Reading data\")\n",
    "    df = pd.read_csv(fn)\n",
    "    # tokenize and binarize cancer classification labels\n",
    "    print(\"Vectorizing labels\")\n",
    "    labelVectorizer = MultiLabelBinarizer()\n",
    "    y = labelVectorizer.fit_transform(df.classifications.str.replace('[\\[\\]\\'\\\"]',\"\").apply(tokenizeCancerLabels))\n",
    "    print(\"Vectorized %d labels\"%y.shape[-1])\n",
    "    X = getFeatures(fn)\n",
    "    return X,y,labelVectorizer.classes_\n",
    "\n",
    "def getFeaturesAndLabelsFine(fn):\n",
    "    '''\n",
    "    Load and vectorizer features and fine grained labels (vectorized using MultiLabelBinarizer)\n",
    "    '''\n",
    "    print(\"Reading data\")\n",
    "    df = pd.read_csv(fn)\n",
    "    # tokenize and binarize cancer classification labels\n",
    "    print(\"Vectorizing labels\")\n",
    "    labelVectorizer = MultiLabelBinarizer()\n",
    "    #y = labelVectorizer.fit_transform(df.classifications.str.replace('[\\[\\]\\'\\\"]',\"\").apply(tokenizeCancerLabels))\n",
    "\n",
    "    from cleaning_classification_labels import clean_labels\n",
    "    y = labelVectorizer.fit_transform(clean_labels.clean_classification(df.classifications.fillna(\"\"), '../data/master/information/translations-labels.csv', 'cleaning_classification_labels/classification_dictionary.csv'))\n",
    "    print(\"Vectorized %d labels\"%y.shape[-1])\n",
    "    X = getFeatures(fn)\n",
    "    return X,y,labelVectorizer.classes_\n",
    "\n",
    "def getFeaturesAndLabelsCoarse(fn):\n",
    "    '''\n",
    "    Load and vectorizer features and coarse grained top level labels (vectorized using MultiLabelBinarizer)\n",
    "    '''\n",
    "    print(\"Reading data\")\n",
    "    df = pd.read_csv(fn)\n",
    "    # tokenize and binarize cancer classification labels\n",
    "    print(\"Vectorizing labels\")\n",
    "    labelVectorizer = MultiLabelBinarizer()\n",
    "    y = labelVectorizer.fit_transform(df.label_top_level.str.replace('[\\[\\]\\'\\\"]',\"\").apply(tokenizeCancerLabels))\n",
    "    print(\"Vectorized %d labels\"%y.shape[-1])\n",
    "    X = getFeatures(fn)\n",
    "    return X,y,labelVectorizer.classes_\n",
    "\n",
    "def getSortedMetrics(true, predicted, labels, scorer):\n",
    "    '''\n",
    "    Scores predictions\n",
    "    '''\n",
    "    score = scorer(true,predicted,average=None)\n",
    "    return [(labels[l],score[l]) for l in score.argsort()[::-1]]\n",
    "\n",
    "\n",
    "def tokenizeCancerLabels(s):\n",
    "    '''\n",
    "    Tokenize the label string and remove empty strings\n",
    "    '''\n",
    "    return [t for t in s.split(\",\") if len(t)>0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_data(path=\"master/features/\"):\n",
    "\n",
    "    def is_hoden_niere(x):\n",
    "        return \"hoden\" in x or \"niere\" in x\n",
    "\n",
    "    data = pd.read_csv(path+\"features.csv\")\n",
    "    correct_label = pd.read_csv(path+\"features-hodenniere.csv\").drop_duplicates([\"pages\",\"fulltitle\"])\n",
    "    c_data = pd.merge(data,correct_label,on=[\"fulltitle\",\"pages\"],how='left')\n",
    "    c_data[\"useful\"] = data.useful.values\n",
    "    is_hoden_niere = c_data[\"cancer_types_x\"].apply(is_hoden_niere)\n",
    "    data[\"useful\"] = c_data.useful.values\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_word_stemmer(x,type_x='porter'):\n",
    "\n",
    "    if type_x not in ['porter','lancaster','snowball']:\n",
    "        return x\n",
    "    words = x.split(\" \")\n",
    "    if type_x == 'porter':\n",
    "        stemmer =  nltk.stem.PorterStemmer()\n",
    "    elif type_x == 'lancaster':\n",
    "        stemmer = nltk.stem.LancasterStemmer()\n",
    "    elif type_x == 'snowball':\n",
    "        stemmer = nltk.stem.SnowballStemmer(language='english')\n",
    "    #print words\n",
    "    return ' '.join([stemmer.stem(w) for w in words])\n",
    "\n",
    "pre_process_word_stemmer('cancers type full',type_x='porter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precisions = []\n",
    "alphas = [1e-3, 1e-2, 1e-1]\n",
    "for a in alphas:\n",
    "    _, ps, _, = classify_cancer('../data/master/features/features.csv', penalty='l2', alpha=a)\n",
    "    print \"alpha: {}, p: {}\".format(a, ps)\n",
    "    precisions.append(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#path_for_translation_labels = 'cancer_data/information/translations-labels.csv'   #path of the .csv translation file provided by the data ambassadors\n",
    "#path_for_manual_transtable = 'classification_dictionary.csv'        # path of the .csv file   that was filled manually by Marie\n",
    "\n",
    "def clean_classification(class_series, trans_path, manual_trans_path):\n",
    "\n",
    "    brac_free = class_series.apply(tokenizeCancerLabels)\n",
    "    \n",
    "    trans_labels_df = pd.read_csv(trans_path,delimiter=';')\n",
    "\n",
    "    labels1st = np.unique(trans_labels_df['Label (1st Level)'])\n",
    "\n",
    "    class_trans_df = pd.read_csv(manual_trans_path,delimiter=';',header=None)\n",
    "\n",
    "    #### create dictionary that translates to correct first level \n",
    "    include_1st_level_dict = {'1-bew': '1-koerper', \n",
    "                              '1-gew': '1-koerper',\n",
    "                              '1-gen': '1-koerper', \n",
    "                              '1-horm':'1-koerper',\n",
    "                              '1-rauch': '1-ps', \n",
    "                              '1-alk': '1-ps', \n",
    "                              '1-canna': '1-ps',\n",
    "                              '1-diab': '1-erk',\n",
    "                              '1-infekt': '1-erk', \n",
    "                              '3-tu-marker': '3-lab',\n",
    "                              '3-biops': '3-lab',\n",
    "                               '5-fr\\xc3\\xbch': '5'}\n",
    "\n",
    "    for label in labels1st:\n",
    "        include_1st_level_dict[label] = label\n",
    "\n",
    "    for i in range(class_trans_df.shape[0]):\n",
    "        cc = class_trans_df.iloc[i]        \n",
    "        include_1st_level_dict[cc[0]] = cc[1]\n",
    "\n",
    "    ## done creating the dictionary\n",
    "    cleaned_series = brac_free.apply(lambda x: clean_levels(x,include_1st_level_dict))\n",
    "\n",
    "    return cleaned_series\n",
    "\n",
    "\n",
    "def tokenizeCancerLabels(s):\n",
    "    '''\n",
    "    Tokenize the label string and remove empty strings\n",
    "    '''\n",
    "    ## if string is an empty list return an empty list\n",
    "    if s == '[]':\n",
    "        return []\n",
    "\n",
    "    ## else return list with    bodypart,classify\n",
    "    s = s.replace(\"['\",\"\").replace(\"']\",\"\")\n",
    "    return [t for t in s.split(\"','\") if len(t)>0]\n",
    "\n",
    "\n",
    "\n",
    "def clean_levels(s_list,trans_dict):\n",
    "    '''\n",
    "    function that cleans the label\n",
    "    '''\n",
    "    new_list = []\n",
    "    ## check if there is an entry\n",
    "    if len(s_list) == 0:\n",
    "        return new_list\n",
    "        \n",
    "    for t in s_list:        \n",
    "        ts = t.split(',')\n",
    "        if len(ts) > 2:  ## here cancer applies to more than one bodyparts.. sort them alphabetically\n",
    "            bodystring = ','.join(sorted(map(lambda x: x.lower(),ts[:-1])))        \n",
    "            old_class = ts[-1]                        \n",
    "        else:   #otherwise there is only one bodypart\n",
    "            bodypart = ts[0]\n",
    "            old_class = ts[1]\n",
    "            \n",
    "        ### correct the classification of the label of format  X-str-...\n",
    "        ocs = old_class.split('-')\n",
    "        \n",
    "        if len(ocs) == 1:\n",
    "            new_class = old_class\n",
    "            \n",
    "        elif len(ocs) == 2:\n",
    "            if trans_dict.has_key(old_class):\n",
    "                new_class = trans_dict[old_class]\n",
    "            else:\n",
    "                new_class = old_class\n",
    "\n",
    "        elif len(ocs) > 2:\n",
    "            first = ocs[0] + '-' + ocs[1]            \n",
    "            first_extended = ocs[0] + '-' + ocs[1] + '-' + ocs[2]  ## need extra check as '3-tu-marker ' should be maped to  '3-labl\n",
    "\n",
    "            if trans_dict.has_key('-'.join(ocs)):\n",
    "                new_class = trans_dict['-'.join(ocs)]\n",
    "            elif trans_dict.has_key(first_extended):\n",
    "                new_class = trans_dict[first_extended]\n",
    "            elif trans_dict.has_key(first):\n",
    "                new_class = trans_dict[first]\n",
    "            else:\n",
    "                new_class = old_class\n",
    "                    \n",
    "        new_list.append(ts[0] + ',' + new_class)\n",
    "\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "from cleaning_classification_labels import clean_labels\n",
    "df = pd.read_csv('../data/master/features/features.csv').fillna(\"\")\n",
    "df['cleaned_classifications'] = clean_labels.clean_classification(df.classifications, '../data/master/information/translations-labels.csv', 'cleaning_classification_labels/classification_dictionary.csv')\n",
    "df[['cleaned_classifications', 'classifications']].head(150)\n",
    "# clean_classification(df, '../data/master/information/translations-labels.csv', 'cleaning_classification_labels/classification_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFramepd.DataFrame({'alpha': alphas, 'precision': precisions}).set_index('alpha')\n",
    "perf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
