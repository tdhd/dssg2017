{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score,precision_score,recall_score,f1_score\n",
    "from scipy.sparse import hstack\n",
    "import warnings,json,gzip\n",
    "\n",
    "\n",
    "\n",
    "def classify_cancer(fn, penalty, alpha):\n",
    "    '''\n",
    "    Runs a multilabel classification experiment\n",
    "    '''\n",
    "    X,y,labelNames = getFeaturesAndLabelsFine(fn)\n",
    "    print X.shape, y.shape, len(labelNames)\n",
    "    # a train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    # turn off warnings, usually there are some labels missing in the training set\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # train a classifier\n",
    "        print(\"Training classifier\")\n",
    "        classif = OneVsRestClassifier(SGDClassifier(penalty=penalty, alpha=alpha), n_jobs=-1).fit(X_train, y_train)\n",
    "    # predict\n",
    "    y_predicted = classif.predict(X_test)\n",
    "    # the scores we want to compute\n",
    "    scorers = [precision_score,recall_score,f1_score]\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # compute Scores\n",
    "        metrics = {s.__name__:getSortedMetrics(y_test,y_predicted,labelNames,s) for s in scorers}\n",
    "    # dump results\n",
    "    json.dump(metrics,gzip.open(\"multilabel_classification_metrics.json\",\"wt\"))\n",
    "    hl = hamming_loss(y_test,y_predicted)\n",
    "    ps = precision_score(y_test, y_predicted, average='samples')\n",
    "    rs = recall_score(y_test, y_predicted, average='samples')\n",
    "    return hl, ps, rs\n",
    "\n",
    "# medicinal database indicators\n",
    "def indicator_for(row):\n",
    "    words = ['medline', 'pubmed', 'embase', 'cochrane', 'cochrane library', 'ovid', 'google scholar']\n",
    "    return [1.0 if row['review_article'] == 1 and word.lower() in row.abstract.lower().split(\" \") else 0.0 for word in words]\n",
    "\n",
    "def indicator_count_for(row):\n",
    "    words = ['medline', 'pubmed', 'embase', 'cochrane', 'cochrane library', 'ovid', 'google scholar']\n",
    "    return np.sum([1.0 if row['review_article'] == 1 and word.lower() in row.abstract.lower().split(\" \") else 0.0 for word in words])\n",
    "\n",
    "def one_hot_database_indicators_from(df):\n",
    "    return OneHotEncoder().fit_transform(df.fillna(\"\").apply(indicator_count_for, axis=1).values.reshape(-1, 1))\n",
    "\n",
    "def getFeatures(fn):\n",
    "    '''\n",
    "    Load and vectorize features\n",
    "    '''\n",
    "    df = pd.read_csv(fn)\n",
    "    features = []\n",
    "    print(\"Vectorizing title character ngrams\")\n",
    "    titleVectorizer = HashingVectorizer(analyzer=\"char_wb\",ngram_range=(1,4),n_features=2**12)\n",
    "    features.append(titleVectorizer.fit_transform(df.fulltitle.fillna(\"\")))\n",
    "    print(\"Vectorizing keywords\")\n",
    "    # searchquery_terms is already a preprocessing step done by data angels, for direct integration\n",
    "    # keywords seems to be better\n",
    "    #features.append(CountVectorizer().fit_transform(df.searchquery_terms.str.replace('[\\[\\]\\'\\\"]',\"\")))\n",
    "    # take original keywords as per search query\n",
    "    features.append(CountVectorizer().fit_transform(df.keywords.str.replace('[\\[\\]\\'\\\"]',\"\")))\n",
    "    print(\"Vectorizing authors\")\n",
    "    features.append(HashingVectorizer(n_features=2**12).fit_transform(df.author.fillna(\"\").str.replace('[\\[\\]\\'\\\"]',\"\")))\n",
    "    print(\"Vectorizing abstracts\")\n",
    "    features.append(HashingVectorizer(n_features=2**12).fit_transform(df.abstract.fillna(\"\").str.replace('[\\[\\]\\'\\\"]',\"\")))\n",
    "    print(\"Computing medicinal database counts\")\n",
    "    features.append(one_hot_database_indicators_from(df))\n",
    "    X = hstack(features)\n",
    "    print(\"Extracted feature vectors with %d dimensions\"%X.shape[-1])\n",
    "    return X\n",
    "\n",
    "def getFeaturesAndLabelsFineMapped(fn):\n",
    "    '''\n",
    "    TODO\n",
    "    Load and vectorizer features and fine grained labels (vectorized using MultiLabelBinarizer)\n",
    "    Before piping through MultiLabelBinarizer, apply mapping to reduce the cardinality of labels\n",
    "    '''\n",
    "    print(\"Reading data\")\n",
    "    df = pd.read_csv(fn)\n",
    "    # tokenize and binarize cancer classification labels\n",
    "    print(\"Vectorizing labels\")\n",
    "    labelVectorizer = MultiLabelBinarizer()\n",
    "    y = labelVectorizer.fit_transform(df.classifications.str.replace('[\\[\\]\\'\\\"]',\"\").apply(tokenizeCancerLabels))\n",
    "    print(\"Vectorized %d labels\"%y.shape[-1])\n",
    "    X = getFeatures(fn)\n",
    "    return X,y,labelVectorizer.classes_\n",
    "\n",
    "def getFeaturesAndLabelsFine(fn):\n",
    "    '''\n",
    "    Load and vectorizer features and fine grained labels (vectorized using MultiLabelBinarizer)\n",
    "    '''\n",
    "    print(\"Reading data\")\n",
    "    df = pd.read_csv(fn)\n",
    "    # tokenize and binarize cancer classification labels\n",
    "    print(\"Vectorizing labels\")\n",
    "    labelVectorizer = MultiLabelBinarizer()\n",
    "    y = labelVectorizer.fit_transform(df.classifications.str.replace('[\\[\\]\\'\\\"]',\"\").apply(tokenizeCancerLabels))\n",
    "    print(\"Vectorized %d labels\"%y.shape[-1])\n",
    "    X = getFeatures(fn)\n",
    "    return X,y,labelVectorizer.classes_\n",
    "\n",
    "def getFeaturesAndLabelsCoarse(fn):\n",
    "    '''\n",
    "    Load and vectorizer features and coarse grained top level labels (vectorized using MultiLabelBinarizer)\n",
    "    '''\n",
    "    print(\"Reading data\")\n",
    "    df = pd.read_csv(fn)\n",
    "    # tokenize and binarize cancer classification labels\n",
    "    print(\"Vectorizing labels\")\n",
    "    labelVectorizer = MultiLabelBinarizer()\n",
    "    y = labelVectorizer.fit_transform(df.label_top_level.str.replace('[\\[\\]\\'\\\"]',\"\").apply(tokenizeCancerLabels))\n",
    "    print(\"Vectorized %d labels\"%y.shape[-1])\n",
    "    X = getFeatures(fn)\n",
    "    return X,y,labelVectorizer.classes_\n",
    "\n",
    "def getSortedMetrics(true, predicted, labels, scorer):\n",
    "    '''\n",
    "    Scores predictions\n",
    "    '''\n",
    "    score = scorer(true,predicted,average=None)\n",
    "    return [(labels[l],score[l]) for l in score.argsort()[::-1]]\n",
    "\n",
    "\n",
    "def tokenizeCancerLabels(s):\n",
    "    '''\n",
    "    Tokenize the label string and remove empty strings\n",
    "    '''\n",
    "    return [t for t in s.split(\",\") if len(t)>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X, y, y_names = getFeaturesAndLabelsFine('../data/master/features/features.csv')\n",
    "# X.shape, y.shape, y_names.shape\n",
    "# X.shape\n",
    "\n",
    "precisions = []\n",
    "alphas = [1e-3, 1e-2, 1e-1]\n",
    "for a in alphas:\n",
    "    _, ps, _, = classify_cancer('../data/master/features/features.csv', penalty='l2', alpha=a)\n",
    "    print \"alpha: {}, p: {}\".format(a, ps)\n",
    "    precisions.append(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cleaning_classification_labels import clean_labels\n",
    "df = pd.read_csv('../data/master/features/features.csv').fillna(\"\")\n",
    "clean_labels.clean_classification(df, '../data/master/information/translations-labels.csv', 'cleaning_classification_labels/classification_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFramepd.DataFrame({'alpha': alphas, 'precision': precisions}).set_index('alpha')\n",
    "perf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
